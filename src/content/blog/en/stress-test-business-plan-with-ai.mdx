Last quarter, a SaaS founder walked into a Series A pitch with a plan he'd spent six weeks polishing. Revenue projections, TAM analysis, competitive positioning, product roadmap. Everything looked tight.

The first VC asked: "Your projections assume 15% month-over-month growth for 18 months. What's your evidence for that after month six, when your current funnel exhausts its initial channels?"

The founder didn't have an answer. The assumption had been in every version of the plan since week one. Nobody had questioned it. Not his co-founder, not his advisor, not the two people he'd asked to review the deck.

This happens more often than founders admit. The plan itself is usually good. The problem is that the people who reviewed it either shared the same assumptions, didn't have time for a deep read, or were too polite to push back on the uncomfortable parts.

## Why traditional feedback fails at stress-testing

Three patterns repeat across industries and contexts.

**Politeness filters criticism.** Your mentor, your colleague, your paid consultant. They all soften their language. "You might want to consider..." means "this won't work." "It could benefit from more detail" means "you don't have evidence for this claim." The translation layer between what they think and what they say strips out the useful signal.

**Busy calendars produce surface reviews.** A proper stress-test of a business plan requires reading every assumption, tracing the logic chain, and checking whether the numbers hold under pressure. That's three to four hours of focused work. Most reviewers give you thirty minutes. They read the executive summary, scan the financials, and give you a thumbs up with two generic suggestions.

**Shared context creates shared blind spots.** If your reviewer works in the same industry, they likely take the same things for granted. The market size assumption that feels obvious to both of you might be the exact thing an outsider would challenge.

## What structured critical analysis looks like

There's a difference between "getting feedback" and running a systematic stress test. A structured analysis doesn't start with opinions. It starts with dimensions.

**Logical flaws and contradictions.** Page 4 claims you'll reach 10,000 users by month 12. Page 7 shows a sales team of two people and no self-serve motion. The math doesn't work, but when you wrote each section separately, the contradiction was invisible.

**Implicit assumptions.** Every plan builds on things the author takes for granted. The market will grow. Customers will switch from the incumbent. The team can ship the roadmap on time. Your technology will work at scale. Each assumption is a load-bearing wall. Remove one and the structure changes.

**Unmapped risks.** Your plan addresses what happens if things go right. A stress test asks what happens when they don't. Key hire leaves. The platform you depend on changes its API. A regulatory shift moves your timeline by six months. These aren't unlikely scenarios. They're Tuesday.

**Uncomfortable questions.** The ones your team avoids because they're politically sensitive or personally uncomfortable. "What happens to the company if you, the founder, can't work for three months?" "Why hasn't anyone solved this problem before, and why do you think you're different?" These questions need answers before an investor or a board member asks them.

**Concrete fixes.** Identifying problems without suggesting solutions is just complaining. A good stress test pairs every finding with a specific, actionable recommendation: validate this assumption with customer interviews, add a contingency plan for this risk, restructure this section to address the logical gap.

## The severity question

Not all findings are equal. A formatting issue on slide 17 doesn't carry the same weight as a flawed TAM calculation that your entire financial model depends on.

A useful analysis tags each finding with a severity level. Critical issues invalidate your core thesis or financials. Medium issues represent solvable gaps that weaken your argument. Low issues are optimization opportunities. This lets you triage: fix the critical ones before the meeting, address the medium ones in your appendix, and file the low ones for later.

## When to run a stress test

The instinct is to wait until the plan is "ready." That's exactly wrong.

Run the first pass after your initial draft. Structural problems are easier to fix before you've spent three weeks polishing the language on top of a flawed foundation. Run another pass before any external presentation. Run a third after any significant pivot, change in strategy, or new market data.

The worst time to discover a critical flaw is when a stakeholder points it out in a room full of people. The best time is 48 hours before, when you still have time to fix it.

## Doing this with Janus

You could run through these dimensions manually. Write down your assumptions, challenge each one, simulate stakeholder perspectives, think about second-order effects. It works. It takes hours, and you're still limited by your own blind spots.

Janus automates the full analysis. Drop in a business plan, a pitch deck, a strategy document, or a proposal. The skill auto-detects which of ten specialized scenarios applies and runs your document through all five analysis dimensions. Every finding gets a severity tag. Every critical finding gets a specific recommendation.

The output is the conversation your skeptical board member would have with you, except you're having it alone, at your desk, with time to prepare your answers.

At 29 euros, it costs less than the espresso budget for the meeting where someone finds the flaw you missed.
